#include 'ppo.gin'
import alf.algorithms.agent
#import alf.algorithms.ppo_algorithm
#import alf.algorithms.ppo_loss

#TrainerConfig.algorithm_ctor=@Agent
#Agent.rl_algorithm_cls=@PPOAlgorithm
#ActorCriticAlgorithm.loss_class=@PPOLoss
#
#observation_spec=@get_observation_spec()
#action_spec=@get_action_spec()
RLAlgorithm.model=True
import alf.algorithms.agent
import alf.algorithms.sac_algorithm

TrainerConfig.algorithm_ctor=@Agent
TrainerConfig.whole_replay_buffer_training=False
TrainerConfig.clear_replay_buffer=False

Agent.rl_algorithm_cls=@SacAlgorithm

observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

import alf.environments.suite_karel_env_hrl

# environment config
create_environment.env_name="Karel"
create_environment.num_parallel_environments=1
create_environment.env_load_fn=@suite_karel_env_hrl.load
create_environment.prl_load_model=True
#create_environment.model_path="/data/jesse/program_prl_exps/skill_prior_learning/program/flat/beta_0.1_2_lstm_layers_nz_vae_5_no_teacher/weights/weights_ep70.pth"
create_environment.model_path="/home/jesse/beta_0.1_2_lstm_layers_nz_vae_5_no_teacher/weights/weights_ep70.pth"
create_environment.n_rollout_steps=20

suite_karel_env_hrl.load.task_definition="custom_reward"
suite_karel_env_hrl.load.width=8
suite_karel_env_hrl.load.height=8
suite_karel_env_hrl.load.env_task="chainSmoker"
suite_karel_env_hrl.load.max_episode_steps=100
suite_karel_env_hrl.load.obv_type="local"
suite_karel_env_hrl.load.wall_prob=0.25
suite_karel_env_hrl.load.incorrect_marker_penalty=True
suite_karel_env_hrl.load.delayed_reward=True

# algorithm config
actor/ActorDistributionRNNNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionRNNNetwork.action_spec=%action_spec
actor/ActorDistributionRNNNetwork.fc_layer_params=(100,)
actor/ActorDistributionRNNNetwork.continuous_projection_net_ctor=@NormalProjectionNetwork

NormalProjectionNetwork.state_dependent_std=True
NormalProjectionNetwork.scale_distribution=True
NormalProjectionNetwork.std_transform=@clipped_exp

critic/CriticRNNNetwork.input_tensor_spec=%observation_spec
critic/CriticRNNNetwork.joint_fc_layer_params=(100,)

SacAlgorithm.actor_optimizer=@actor/Adam()
SacAlgorithm.critic_optimizer=@critic/Adam()
SacAlgorithm.alpha_optimizer=@alpha/Adam()
SacAlgorithm.actor_network_cls=@actor/ActorDistributionRNNNetwork
SacAlgorithm.use_parallel_network=True
SacAlgorithm.critic_network_cls=@critic/CriticRNNNetwork
SacAlgorithm.target_update_tau=0.005
OneStepTDLoss.td_error_loss_fn=@losses.element_wise_squared_loss

# training config
TrainerConfig.mini_batch_length=2
TrainerConfig.unroll_length=8
TrainerConfig.mini_batch_size=128
TrainerConfig.num_updates_per_train_iter=8
TrainerConfig.num_iterations=0
TrainerConfig.num_env_steps=300000
TrainerConfig.evaluate=True
TrainerConfig.eval_interval=100
TrainerConfig.debug_summaries=False
TrainerConfig.summary_interval=15
TrainerConfig.epsilon_greedy=0.
TrainerConfig.num_checkpoints=1
TrainerConfig.num_eval_episodes=20
TrainerConfig.replay_buffer_length=1000000


