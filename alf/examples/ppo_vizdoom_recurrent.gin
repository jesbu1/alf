#include 'ppo.gin'
import alf.algorithms.agent
import alf.algorithms.ppo_algorithm
import alf.algorithms.ppo_loss

TrainerConfig.algorithm_ctor=@Agent
Agent.rl_algorithm_cls=@PPOAlgorithm
ActorCriticAlgorithm.loss_class=@PPOLoss

observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

import alf.environments.suite_vizdoom_env

# environment config
create_environment.env_name="VizDoom"
create_environment.num_parallel_environments=10
create_environment.env_load_fn=@suite_vizdoom_env.load

suite_vizdoom_env.load.task_definition="custom_reward"
suite_vizdoom_env.load.max_episode_steps=200
#suite_vizdoom_env.load.env_task="survive"
#suite_vizdoom_env.load.vizdoom_config_file="vizdoom_env/asset/default.cfg"
suite_vizdoom_env.load.env_task="preloaded"
suite_vizdoom_env.load.vizdoom_config_file="vizdoom_env/asset/scenarios/defend_the_center.cfg"
suite_vizdoom_env.load.obv_type="local"

# algorithm config
actor/ActorDistributionRNNNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionRNNNetwork.action_spec=%action_spec
actor/ActorDistributionRNNNetwork.fc_layer_params=(100,)
#actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
#actor/ActorDistributionNetwork.action_spec=%action_spec
#actor/ActorDistributionNetwork.fc_layer_params=(100,100,)

value/ValueRNNNetwork.input_tensor_spec=%observation_spec
value/ValueRNNNetwork.fc_layer_params=(100,)
#value/ValueNetwork.input_tensor_spec=%observation_spec
#value/ValueNetwork.fc_layer_params=(100,100,)

ac/Adam.lr=3e-4

ActorCriticAlgorithm.actor_network_ctor=@actor/ActorDistributionRNNNetwork
ActorCriticAlgorithm.value_network_ctor=@value/ValueRNNNetwork
#ActorCriticAlgorithm.actor_network_ctor=@actor/ActorDistributionNetwork
#ActorCriticAlgorithm.value_network_ctor=@value/ValueNetwork
Agent.optimizer=@ac/Adam()


PPOLoss.entropy_regularization=0.01
PPOLoss.gamma=0.99
PPOLoss.td_error_loss_fn=@element_wise_huber_loss
PPOLoss.normalize_advantages=True
PPOLoss.importance_ratio_clipping=0.3

# training config
TrainerConfig.mini_batch_length=None
TrainerConfig.unroll_length=100
TrainerConfig.mini_batch_size=128
TrainerConfig.num_updates_per_train_iter=20
TrainerConfig.num_iterations=0
TrainerConfig.num_env_steps=1000000
TrainerConfig.evaluate=True
TrainerConfig.eval_interval=15
TrainerConfig.num_eval_episodes=10
TrainerConfig.debug_summaries=False
TrainerConfig.summary_interval=5
TrainerConfig.summarize_action_distributions=True
TrainerConfig.summarize_output=True
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.epsilon_greedy=0.
TrainerConfig.num_checkpoints=1
TrainerConfig.use_rollout_state=True

ReplayBuffer.max_length = 2048